{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81d38d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YinJianxiang\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YinJianxiang\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YinJianxiang\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "out = [0.9482299 0.        1.1610144 2.747859  0.        1.36677  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "import resnets_utils \n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    实现恒等块\n",
    "    参数：\n",
    "    X:输入数据,维度(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f:第二部分卷积层的维度\n",
    "    filters:每部分卷积层的过滤器个数\n",
    "    stage:命名参数，层数\n",
    "    block:命名参数，层的名字\n",
    "    返回值：\n",
    "    X_out:恒等快输出，维度(n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "    \n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #第一部分\n",
    "    X = Conv2D(filters=F1, \n",
    "               kernel_size=(1,1), \n",
    "               strides=(1,1),\n",
    "               padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #第二部分\n",
    "    X = Conv2D(filters=F2, \n",
    "               kernel_size=(f,f),\n",
    "               strides=(1,1), \n",
    "               padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    \n",
    "    #第三部分\n",
    "    X = Conv2D(filters=F3,\n",
    "               kernel_size=(1,1), \n",
    "               strides=(1,1),\n",
    "               padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X_output = Activation(\"relu\")(X)\n",
    "\n",
    "    return X_output\n",
    "\n",
    "\n",
    "#test 恒等快\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\",[3,4,4,6])\n",
    "    X = np.random.randn(3,4,4,6)\n",
    "    A = identity_block(A_prev,f=2,filters=[2,4,6],stage=1,block=\"a\")\n",
    "    \n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A],feed_dict={A_prev:X,K.learning_phase():0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))\n",
    "    \n",
    "    test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7adfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [0.09018461 1.2348977  0.46822017 0.0367176  0.         0.655166  ]\n"
     ]
    }
   ],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    实现跳跃三层的恒等快\n",
    "    参数：\n",
    "    X:输入数据,维度(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f:第二部分卷积层的维度\n",
    "    filters:每部分卷积层的过滤器个数\n",
    "    stage:命名参数，层数\n",
    "    block:命名参数，层的名字\n",
    "    s:第一部分、第三部分、X_shortcut的stride\n",
    "    返回值：\n",
    "    X_out:恒等快输出，维度(n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    #第一部分\n",
    "    X = Conv2D(filters = F1, \n",
    "               kernel_size=(1,1),\n",
    "               strides=(s,s),\n",
    "               padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\",\n",
    "               kernel_initializer=glorot_uniform(seed=0)\n",
    "              )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #第二部分\n",
    "    X = Conv2D(filters = F2, \n",
    "               kernel_size=(f,f),\n",
    "               strides=(1,1),\n",
    "               padding=\"same\",\n",
    "               name=conv_name_base+\"2b\",\n",
    "               kernel_initializer=glorot_uniform(seed=0)\n",
    "              )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #第三部分\n",
    "    X = Conv2D(filters = F3, \n",
    "               kernel_size=(1,1),\n",
    "               strides=(1,1),\n",
    "               padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\",\n",
    "               kernel_initializer=glorot_uniform(seed=0)\n",
    "              )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+\"2c\")(X)\n",
    "    \n",
    "    #shortcut\n",
    "    X_shortcut = Conv2D(filters = F3, \n",
    "               kernel_size=(1,1),\n",
    "               strides=(s,s),\n",
    "               padding=\"valid\",\n",
    "               name=conv_name_base+\"1\",\n",
    "               kernel_initializer=glorot_uniform(seed=0)\n",
    "              )(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base+\"1\")(X_shortcut)\n",
    "    \n",
    "    #求和\n",
    "    X = Add()([X, X_shortcut])\n",
    "    #激活\n",
    "    X_output = Activation(\"relu\")(X)\n",
    "    \n",
    "    return X_output    \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\",[3,4,4,6])\n",
    "    X = np.random.randn(3,4,4,6)\n",
    "    \n",
    "    A = convolutional_block(A_prev,f=2,filters=[2,4,6],stage=1,block=\"a\")\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    \n",
    "    out = test.run([A],feed_dict={A_prev:X,K.learning_phase():0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))\n",
    "    \n",
    "    test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f1f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape, classes):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    实现resnet50残差网络\n",
    "    参数:\n",
    "    input_shape:输入数据的维度(n_H,n_W,n_C)\n",
    "    classes:分类种类\n",
    "    返回：\n",
    "    net:残差网络模型\n",
    "    \"\"\"\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #填充\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    #stage 1\n",
    "    X = Conv2D(filters = 64, \n",
    "               kernel_size=(7,7),\n",
    "               strides=(2,2),\n",
    "               name=\"conv1\",\n",
    "               kernel_initializer=glorot_uniform(seed=0)\n",
    "              )(X)\n",
    "    X = BatchNormalization(axis=3, name=\"bn_conv1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = MaxPooling2D(pool_size=(3,3), strides=(2,2))(X)\n",
    "\n",
    "    #stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64,64,256], stage=2, block=\"a\", s=1)\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"c\")\n",
    "    \n",
    "    #stage3\n",
    "    X = convolutional_block(X, f=3, filters=[128,128,512], stage=3, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"d\")\n",
    "    \n",
    "    #stage4\n",
    "    X = convolutional_block(X, f=3, filters=[256,256,1024], stage=4, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"d\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"e\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"f\")\n",
    "    \n",
    "    #stage5\n",
    "    X = convolutional_block(X, f=3, filters=[512,512,2048], stage=5, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"c\")\n",
    "    \n",
    "    #均匀池化\n",
    "    X = AveragePooling2D(pool_size=(2,2), padding=\"same\", name=\"avg_pool\")(X) \n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, \n",
    "              activation=\"softmax\", \n",
    "              name=\"fc\"+str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0)\n",
    "             )(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name = \"ResNet\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = ResNet50(input_shape=(64,64,3), classes=6)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b03d13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n",
      "Train on 1080 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,512,8,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/gradients/zeros_174}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[metrics_2/acc/Identity/_2235]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,512,8,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/gradients/zeros_174}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f1954b1aebab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Y_test shape: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m     return fit_loop(\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4029\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4031\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   4032\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   4033\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1476\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1479\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,512,8,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/gradients/zeros_174}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[metrics_2/acc/Identity/_2235]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,512,8,8] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/gradients/zeros_174}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = resnets_utils.load_dataset()\n",
    "\n",
    "X_train = X_train_orig / 255\n",
    "X_test = X_test_orig / 255\n",
    "\n",
    "Y_train = resnets_utils.convert_to_one_hot(Y_train_orig,6).T\n",
    "Y_test = resnets_utils.convert_to_one_hot(Y_test_orig,6).T\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=2,batch_size=32)\n",
    "preds = model.evaluate(X_test,Y_test)\n",
    "\n",
    "print(\"误差值 = \" + str(preds[0]))\n",
    "print(\"准确率 = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76143226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
