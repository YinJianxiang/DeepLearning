{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ae556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 4831.929689202033\n",
      "199 3275.474959028179\n",
      "299 2223.6535142922944\n",
      "399 1512.1280324370045\n",
      "499 1030.3032978073493\n",
      "599 703.6800300254064\n",
      "699 482.0273729942425\n",
      "799 331.4450182313207\n",
      "899 229.03153453688395\n",
      "999 159.30024314781744\n",
      "1099 111.76777655338714\n",
      "1199 79.33018883726987\n",
      "1299 57.168407294679575\n",
      "1399 42.00977755502011\n",
      "1499 31.62936857432123\n",
      "1599 24.512856824502933\n",
      "1699 19.628404565186457\n",
      "1799 16.272131793204316\n",
      "1899 13.963328593477577\n",
      "1999 12.373320177386764\n",
      "Result: y = 0.05117064437243873 + 0.8227998466414344 x + -0.008827787637443402 x^2 + -0.08850257785889436 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#使用numpy实现神经网络\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7059e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6095.591796875\n",
      "199 4116.40087890625\n",
      "299 2783.484619140625\n",
      "399 1884.982666015625\n",
      "499 1278.7406005859375\n",
      "599 869.2943115234375\n",
      "699 592.4845581054688\n",
      "799 405.15325927734375\n",
      "899 278.2442321777344\n",
      "999 192.17706298828125\n",
      "1099 133.74461364746094\n",
      "1199 94.03007507324219\n",
      "1299 67.00733947753906\n",
      "1399 48.5997314453125\n",
      "1499 36.04637908935547\n",
      "1599 27.475698471069336\n",
      "1699 21.61733627319336\n",
      "1799 17.608327865600586\n",
      "1899 14.86177921295166\n",
      "1999 12.977968215942383\n",
      "Result: y = -0.052901774644851685 + 0.817099928855896 x + 0.009126436896622181 x^2 + -0.08769181370735168 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "#PyTorch Tensor 在概念上与 numpy 数组相同：Tensor 是一个 n 维数组，PyTorch 提供了许多操作这些 Tensor 的函数。\n",
    "#PyTorch Tensors 可以利用 GPU 来加速其数值计算。要在 GPU 上运行 PyTorch Tensor，您只需指定正确的设备。\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8eccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1420.5611572265625\n",
      "199 942.403076171875\n",
      "299 626.2010498046875\n",
      "399 417.09765625\n",
      "499 278.81787109375\n",
      "599 187.372802734375\n",
      "699 126.89958190917969\n",
      "799 86.9080810546875\n",
      "899 60.4609489440918\n",
      "999 42.971092224121094\n",
      "1099 31.40462303161621\n",
      "1199 23.755273818969727\n",
      "1299 18.696590423583984\n",
      "1399 15.35107707977295\n",
      "1499 13.138484954833984\n",
      "1599 11.675180435180664\n",
      "1699 10.707425117492676\n",
      "1799 10.067357063293457\n",
      "1899 9.644065856933594\n",
      "1999 9.364091873168945\n",
      "Result: y = -0.001541219069622457 + 0.8340513110160828 x + 0.00026588552282191813 x^2 + -0.09010300040245056 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "#使用PyTorch中的 autograd包自动计算反向传播梯度\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "#y = a + b x + c x^2 + d x^3\n",
    "a = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "b = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "c = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "d = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    #使用autograd来计算向后传递\n",
    "    #这个调用将计算关于requires_grad=True的所有张量的损失梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    #torch.no_grad()  不跟踪梯度变化\n",
    "    #由于在更新权重时候不需要跟踪梯度变化    \n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        # 更新权重后，手动将梯度置零,否则会叠加之前的值\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f744fc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.973188400268555\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -5.423830273798558e-09 + -2.208526849746704 * P3(1.3320399228078372e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 通过定义torch.autograd.Function和实现forward和backward函数的子类来轻松定义 autograd 运算符。\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    ctx是context的缩写，上下文，环境\n",
    "    ctx专门用在静态方法中，调用不需要实例化对象，直接通过类名就可以调用\n",
    "    自定义的forward()方法和backward()方法的第一个参数必须是ctx; ctx可以保存forward()中的变量,以便在backward()中继续使用\n",
    "    ctx.save_for_backward(a, b)能够保存forward()静态方法中的张量, 从而可以在backward()静态方法中调用, 具体地, 通过a, b = ctx.saved_tensors重新得到a和b\n",
    "    ctx.needs_input_grad是一个元组, 元素是True或者False, 表示forward()中对应的输入是否需要求导\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input): \n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grag_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grag_output * 1.5 * (5 * input ** 2 - 1)\n",
    "        \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.full((), 0.0, device = device, dtype = dtype,requires_grad = True)\n",
    "b = torch.full((), -1.0, device = device, dtype = dtype,requires_grad = True)\n",
    "c = torch.full((), 0.0, device = device, dtype = dtype,requires_grad = True)\n",
    "d = torch.full((), 0.3, device = device, dtype = dtype,requires_grad = True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    #apply Fuction\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1573239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 133.08131408691406\n",
      "199 91.0268783569336\n",
      "299 63.20665740966797\n",
      "399 44.80228042602539\n",
      "499 32.62653732299805\n",
      "599 24.571210861206055\n",
      "699 19.241649627685547\n",
      "799 15.715407371520996\n",
      "899 13.382174491882324\n",
      "999 11.838301658630371\n",
      "1099 10.816669464111328\n",
      "1199 10.140596389770508\n",
      "1299 9.69316577911377\n",
      "1399 9.397064208984375\n",
      "1499 9.201065063476562\n",
      "1599 9.071333885192871\n",
      "1699 8.985456466674805\n",
      "1799 8.928606986999512\n",
      "1899 8.890969276428223\n",
      "1999 8.866048812866211\n",
      "Result: y = -0.0011633113026618958 + 0.8500290513038635 x + 0.000200691181817092 x^2 + -0.09237569570541382 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "#nn包定义了一组Modules，大致相当于神经网络层。模块接收输入张量并计算输出张量，但也可以保存内部状态，例如包含可学习参数的张量。\n",
    "#nn包还定义了一组在训练神经网络时常用的有用的损失函数\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "#y = x.unsqueeze(-1)\n",
    "#print(y.shape)\n",
    "\n",
    "#广播 (2000,1)  (3，) -> (2000,3)\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "\n",
    "#nn.Sequential\n",
    "#一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行\n",
    "#同时以神经网络模块为元素的有序字典也可以作为传入参数。\n",
    "#Linear y = xAT + b使用线性函数计算输入的输出，并保存其权重和偏差的内部张量\n",
    "#Linear(in_features, out_features, bias = True)\n",
    "#输入特征数，输出特征数\n",
    "#Flatten(x,y) 从x维到y维推平，保证输出层1维tensor，匹配y\n",
    "model = torch.nn.Sequential (\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "#m = torch.nn.Linear(3, 1)\n",
    "#print(m.weight.shape)\n",
    "#output = m(xx)\n",
    "#print(output.shape)\n",
    "#Linear生成(1，3)矩阵，在转置为(3,1)\n",
    "\n",
    "#选择MSE计算损失函数 y = (1/m) sum(y - y')^2\n",
    "#reduction sum or mean\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "\n",
    "#preds = model(inputs)             ## inference\n",
    "#loss = criterion(preds, targets)  ## 求解loss\n",
    "#optimizer.zero_grad()             ## 梯度清零\n",
    "#loss.backward()                   ## 反向传播求解梯度\n",
    "#optimizer.step()                  ## 更新权重参数\n",
    "\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    #梯度清0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    #反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f69fe7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3007.56982421875\n",
      "199 1284.267578125\n",
      "299 1019.1074829101562\n",
      "399 868.1571044921875\n",
      "499 714.6934814453125\n",
      "599 569.8775024414062\n",
      "699 441.0660705566406\n",
      "799 330.3569641113281\n",
      "899 237.97438049316406\n",
      "999 163.3916778564453\n",
      "1099 105.56173706054688\n",
      "1199 63.303375244140625\n",
      "1299 35.096439361572266\n",
      "1399 18.800708770751953\n",
      "1499 11.385660171508789\n",
      "1599 9.182021141052246\n",
      "1699 8.882740020751953\n",
      "1799 8.926872253417969\n",
      "1899 8.942255973815918\n",
      "1999 8.914430618286133\n",
      "Result: y = -0.0005012111505493522 + 0.8571780323982239 x + -0.0005012317560613155 x^2 + -0.09284719079732895 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "#optim包中有优化函数算法，并提供了常用优化算法的实现。\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b83a3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1376.782958984375\n",
      "199 916.1348876953125\n",
      "299 610.727294921875\n",
      "399 408.20770263671875\n",
      "499 273.8888854980469\n",
      "599 184.78555297851562\n",
      "699 125.66439056396484\n",
      "799 86.42768859863281\n",
      "899 60.38158416748047\n",
      "999 43.087039947509766\n",
      "1099 31.600505828857422\n",
      "1199 23.96929168701172\n",
      "1299 18.897789001464844\n",
      "1399 15.52632999420166\n",
      "1499 13.284313201904297\n",
      "1599 11.792794227600098\n",
      "1699 10.800185203552246\n",
      "1799 10.13931655883789\n",
      "1899 9.699146270751953\n",
      "1999 9.40582275390625\n",
      "Result: y = 0.009646207094192505 + 0.8348827958106995 x + -0.001664131530560553 x^2 + -0.09022127091884613 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "#自定义nn模型\n",
    "#我们需要更复杂的模块， 我们可以通过 继承 nn.Module  \n",
    "#和定义 forward（用来接收Input tensor和输出output tensor）\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        实例化四个参数，并指定为成员参数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "    \n",
    "    def string(self):\n",
    "                return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "        \n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "model = Polynomial3()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr= 1e-6)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    loss= criterion(y_pred,y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a55b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 525.9119873046875\n",
      "3999 251.20989990234375\n",
      "5999 127.77450561523438\n",
      "7999 66.87577819824219\n",
      "9999 36.3028678894043\n",
      "11999 23.155181884765625\n",
      "13999 15.880390167236328\n",
      "15999 12.707060813903809\n",
      "17999 10.614063262939453\n",
      "19999 9.727209091186523\n",
      "21999 9.292576789855957\n",
      "23999 9.08722972869873\n",
      "25999 8.962778091430664\n",
      "27999 8.662066459655762\n",
      "29999 8.909375190734863\n",
      "Result: y = 0.006035753525793552 + 0.8568557500839233 x + -0.0016227232990786433 x^2 + -0.0937441810965538 x^3 + 0.00012499438889790326 x^4 ? + 0.00012499438889790326 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "\n",
    "#动态图和权重共享\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x **3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y += self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "    \n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = DynamicNet()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-8, momentum = 0.9)\n",
    "\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1237ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
